{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "import os\n",
    "#os.chdir(path = {your path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basics\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nlp relevants\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# for bag-of-words (bow)\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "data = pd.read_json('posts.json')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL:\n",
    "\n",
    "    # text normalization - stemming, lemmatization, stopwords\n",
    "    ps = PorterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer() \n",
    "    s_words = stopwords.words()\n",
    "    \n",
    "    \n",
    "    # normalization of question sentences\n",
    "    def _norm_sent(self, sent, rm_stopwords = False, stemming = True, lemmatization = False):\n",
    "        \n",
    "        # tokenize - sentence to word\n",
    "        words = word_tokenize(sent)\n",
    "        \n",
    "        # take if all characters in the string are alphabets and then decapitalize\n",
    "        sent = [w.lower() for w in words if w.isalpha()] \n",
    "\n",
    "        # remove stopwords\n",
    "        if rm_stopwords:\n",
    "          sent = [w for w in sent if w not in self.s_words]    \n",
    "\n",
    "        # apply lemmatization \n",
    "        if lemmatization:\n",
    "          sent = [self.wordnet_lemmatizer.lemmatize(w, pos = \"n\") for w in sent]\n",
    "          sent = [self.wordnet_lemmatizer.lemmatize(w, pos = \"v\") for w in sent]\n",
    "          sent = [self.wordnet_lemmatizer.lemmatize(w, pos = (\"a\")) for w in sent]\n",
    "\n",
    "        # apply stemming \n",
    "        if stemming:\n",
    "          sent = [self.ps.stem(w) for w in sent]\n",
    "\n",
    "        sent = \" \".join(sent)\n",
    "        return sent  \n",
    "    \n",
    "    \n",
    "    def norm_data(self, data):   \n",
    "        data.loc[:, \"title_processed\"] = data[\"title\"].apply(lambda x: self._norm_sent(x, rm_stopwords = True, lemmatization = True, stemming = True))\n",
    "        return data   \n",
    "    \n",
    "    \n",
    "    def bow_fit(self, corpus, type = \"tfidf\", max_features = 10000, ngram_range = (1,2)):\n",
    "        \n",
    "        if type == \"tfidf\": \n",
    "            self.tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(max_features = max_features, ngram_range = ngram_range)\n",
    "            self.tfidf_vectorizer.fit(corpus[\"title\"])\n",
    "\n",
    "            # create a reverse mapping for the vocab\n",
    "            self.inv_tfidf_vectorizer_vocab = {}\n",
    "            \n",
    "            for label, ind in self.tfidf_vectorizer.vocabulary_.items():\n",
    "                self.inv_tfidf_vectorizer_vocab[ind] = label\n",
    "\n",
    "        else:\n",
    "            return NotImplementedError\n",
    "        \n",
    "        \n",
    "    def bow_transform(self, data, type = \"tfidf\"):\n",
    "        \n",
    "        if type == \"tfidf\":\n",
    "            return self.tfidf_vectorizer.transform(data[\"title\"])\n",
    "        \n",
    "        else:\n",
    "            return NotImplementedError\n",
    "\n",
    "    # save output\n",
    "    def save_vectorizers(self, path):\n",
    "\n",
    "        # make sure directory exists\n",
    "        os.makedirs(exist_ok= True, name=path)\n",
    "\n",
    "        if self.tfidf_vectorizer != None:\n",
    "            with open(os.path.join(path, \"tfidf_vectorizer.pkl\"), \"wb\") as tfidf_file:\n",
    "                pickle.dump(self.tfidf_vectorizer, tfidf_file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl = ETL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = etl.norm_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization - bag of words model\n",
    "etl.bow_fit(corpus = df, type = \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl.save_vectorizers(path=\"sklearn_objects\")\n",
    "\n",
    "def retrieve(query: str):\n",
    "    query = etl._norm_sent(query, rm_stopwords = True, lemmatization = True, stemming = True)\n",
    "    query = etl.tfidf_vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query, etl.bow_transform(df, type = \"tfidf\"))\n",
    "    scores = scores.flatten()\n",
    "    return df.iloc[np.argsort(-scores)[:10]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment-forum-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "48840acd2f49d1e6e84996b0410ba4f080e3d95a872c8fd63947baefb0c76d26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
